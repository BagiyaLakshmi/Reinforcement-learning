{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7033b92a-3bfd-45fc-8563-2f107d678ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f452d1f9-505a-40c8-971e-4d1c0ffd5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_size = 100\n",
    "start = (0, 0)\n",
    "goal = (99, 99)\n",
    "obstacle_percentage = 0.2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771d98f8-94e1-4fb7-9be9-26c2ddf6ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the grid environment\n",
    "grid = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# Randomly place obstacles\n",
    "num_obstacles = int(obstacle_percentage * grid_size * grid_size)\n",
    "for _ in range(num_obstacles):\n",
    "    x, y = random.randint(0, grid_size-1), random.randint(0, grid_size-1)\n",
    "    grid[x, y] = -1  # -1 represents an obstacle\n",
    "\n",
    "# Ensure start and goal are not obstacles\n",
    "grid[start] = 0\n",
    "grid[goal] = 0\n",
    "\n",
    "# Define MDP parameters\n",
    "actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # Right, Down, Left, Up\n",
    "gamma = 0.9  # Discount factor\n",
    "reward_goal = 10\n",
    "reward_step = -0.1\n",
    "reward_obstacle = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c0f365-92a4-4939-ad87-56e85a000c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values = np.zeros((grid_size, grid_size))\n",
    "\n",
    "def is_valid_state(state):\n",
    "    x, y = state\n",
    "    return 0 <= x < grid_size and 0 <= y < grid_size and grid[x, y] != -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832a5bee-a400-4460-a538-3fba2d2ec8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next_state(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = action\n",
    "    next_state = (x + dx, y + dy)\n",
    "    return next_state if is_valid_state(next_state) else state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716f6a5b-f0b3-42a1-a5d2-f4b812e457fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Value Iteration\n",
    "def value_iteration(threshold=1e-4):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                state = (x, y)\n",
    "                if state == goal:\n",
    "                    continue  \n",
    "\n",
    "                v = values[state]\n",
    "                values[state] = max(\n",
    "                    reward_step + gamma * values[get_next_state(state, a)]\n",
    "                    for a in actions\n",
    "                )\n",
    "                delta = max(delta, abs(v - values[state]))\n",
    "\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "# Q-learning parameters\n",
    "q_values = np.zeros((grid_size, grid_size, len(actions)))\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17226f1-7fbf-473d-a5d4-bf0e6f7124fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Value Iteration...\n",
      "Value Iteration completed.\n",
      "Running Q-learning...\n",
      "Q-learning completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def q_learning():\n",
    "    for episode in range(num_episodes):\n",
    "        state = start\n",
    "        while state != goal:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action_idx = random.randint(0, len(actions) - 1)  # Explore\n",
    "            else:\n",
    "                action_idx = np.argmax(q_values[state[0], state[1]])  # Exploit\n",
    "\n",
    "            action = actions[action_idx]\n",
    "            next_state = get_next_state(state, action)\n",
    "\n",
    "            reward = reward_goal if next_state == goal else reward_step\n",
    "            q_values[state[0], state[1], action_idx] += alpha * (\n",
    "                reward + gamma * np.max(q_values[next_state[0], next_state[1]]) -\n",
    "                q_values[state[0], state[1], action_idx]\n",
    "            )\n",
    "            state = next_state\n",
    "\n",
    "# Run Value Iteration and Q-learning, then compare\n",
    "print(\"Running Value Iteration...\")\n",
    "value_iteration()\n",
    "print(\"Value Iteration completed.\")\n",
    "print(\"Running Q-learning...\")\n",
    "q_learning()\n",
    "print(\"Q-learning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7e7adb-0257-4144-bdd8-640c68f82dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Q-learning):\n",
      "[['→' '↓' '↓' ... '→' '→' '↓']\n",
      " ['→' '↓' '→' ... 'X' '↑' '←']\n",
      " ['←' '↓' '↓' ... '↓' '↓' '→']\n",
      " ...\n",
      " ['←' '↓' '→' ... '→' '→' '↓']\n",
      " ['↓' '←' '↑' ... 'X' '→' '↓']\n",
      " ['↓' 'X' '←' ... 'X' '→' 'G']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_optimal_policy():\n",
    "    policy_grid = np.zeros((grid_size, grid_size), dtype=str)\n",
    "    for x in range(grid_size):\n",
    "        for y in range(grid_size):\n",
    "            if (x, y) == goal:\n",
    "                policy_grid[x, y] = \"G\"\n",
    "            elif grid[x, y] == -1:\n",
    "                policy_grid[x, y] = \"X\"\n",
    "            else:\n",
    "                best_action = np.argmax(q_values[x, y])\n",
    "                if best_action == 0:\n",
    "                    policy_grid[x, y] = \"→\"\n",
    "                elif best_action == 1:\n",
    "                    policy_grid[x, y] = \"↓\"\n",
    "                elif best_action == 2:\n",
    "                    policy_grid[x, y] = \"←\"\n",
    "                elif best_action == 3:\n",
    "                    policy_grid[x, y] = \"↑\"\n",
    "    print(policy_grid)\n",
    "\n",
    "print(\"Optimal Policy (Q-learning):\")\n",
    "print_optimal_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6c972-d7ec-4d8f-9f24-83a056c6b873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
